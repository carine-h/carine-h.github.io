---
title: "blog_9"
author: "Carine Hajjar"
date: "11/16/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(tidyverse)
library(ggplot2)
library(statebins)
library(stargazer)
library(cowplot)
library(readr)
library(reshape2)
library(statebins)
library(jtools)
library(modelr)
library(gt)

knitr::opts_chunk$set(echo = TRUE)

popvote_df    <- read_csv("/Users/carinehajjar/Desktop/R studio/carine-h.github.io/data/popvote_1948-2016.csv")
# 538
pvstate_df    <- read_csv("/Users/carinehajjar/Desktop/R studio/carine-h.github.io/data/popvote_bystate_1948-2016.csv")
economy_df    <- read_csv("/Users/carinehajjar/Desktop/R studio/carine-h.github.io/data/econ.csv")
approval_df   <- read_csv("/Users/carinehajjar/Desktop/R studio/carine-h.github.io/data/approval_gallup_1941-2020.csv")
# 538
poll_state_df  <- read_csv("/Users/carinehajjar/Desktop/R studio/carine-h.github.io/data/pollavg_bystate_1968-2016.csv")
state_pv_df<- read_csv("/Users/carinehajjar/Desktop/R studio/carine-h.github.io/pred_data/popvote_bystate_1948-2016 copy 2.csv")
app <- read_csv("/Users/carinehajjar/Desktop/R studio/carine-h.github.io/data/approval_gallup_1941-2020.csv")
# 538: 2020 poll averages per state 
poll_2020_state <- read_csv("/Users/carinehajjar/Desktop/R studio/carine-h.github.io/pred_data/presidential_poll_averages_2020.csv")
econ_update <- read_csv("/Users/carinehajjar/Desktop/R studio/carine-h.github.io/pred_data/econ_update.csv")
demog <- read_csv("~/Desktop/R studio/carine-h.github.io/data/demographic_1990-2018.csv")
pvstate_df    <- read_csv("~/Desktop/R studio/carine-h.github.io/data/popvote_bystate_1948-2016.csv")
hispanic_2020 <- read_csv("~/Desktop/R studio/carine-h.github.io/data/csvData.csv")
race_2020 <- read_csv("~/Desktop/R studio/carine-h.github.io/data/demog_race.csv")
electoral_votes <- read_csv("~/Desktop/R studio/carine-h.github.io/data/electoralcollegevotes_1948-2020.csv")
econ_update <- read_csv("~/Desktop/R studio/carine-h.github.io/pred_data/SAGDP1__ALL_AREAS_1997_2019.csv")
outcome_2020 <- read_csv("/Users/carinehajjar/Desktop/R studio/carine-h.github.io/data/popvote_bystate_1948-2020.csv") %>%
  filter(year == 2020) %>%
  na.omit()



# tidyinig electoral votes for my purposes 
electoral <- electoral_votes %>%
  select(X1, '2020') %>%
  rename(state = X1) %>%
  mutate(state = state.abb[match(state,state.name)]) %>%
  na.omit() 
```

CHECK 1. A recap of your model(s) and your predictions 
HOW 2. A description of the accuracy of the model(s), including any apparent patterns in the accuracy. Graphics should be used here.
KINDA 3. Proposed hypotheses for why the model(s) were inaccurate in the estimates or locations where it was inaccurate.  These reasons should not simply be statements of about the quality of the components of the model, e.g., “the polls were not good” or “economic growth was not a good predictor” but should instead be grounded hypotheses on why components of the model may not have been predictive or may not have been predictive in certain cases.
4. Proposed quantitative tests that could test these hypotheses, e.g., what data, if available, could allow you to test whether the reason proposed really did cause the inaccuracy in your model.  If there is no plausible test of the hypothesis, explain why.  You do not need to perform these tests or explain them in great detail (e.g., there is no need to write down an equation showing your exact test), just propose them.  
5. A description of how you might change your model if you were to do it again.  

# Recap of Model and Predictions 
The week before the election I created a predictive model that placed Biden as the victor with **372** electoral votes and Trump with **163.** Here's my state-by-state prediction: 

MAP

```{r}
#############################
#### POLL MODEL and PRED ####
#############################
dat <- state_pv_df %>%
  filter(state != "District of Columbia") %>% 
  full_join(poll_state_df %>% 
              filter(weeks_left <= 10) %>% 
              group_by(year,party,state) %>% 
              summarise(avg_poll=mean(avg_poll)),
            by = c("year" ,"state")) %>%
   filter(state != "District of Columbia", 
         state != "ME-1", 
         state != "ME-2",
         state != "NE-1", 
         state != "NE-2",
         state != "NE-3",
         state != "National", 
         party == "democrat") 

# MODEL
fit_state_poll <- lm(D_pv2p ~ avg_poll + as.factor(state), data = dat)
summary(fit_state_poll)
export_summs(fit_state_poll)
## prediction : take the average for each state
    # average of three weeks ago: October 8
new_data_poll <- poll_2020_state %>%
  filter(candidate_name %in% c("Joseph R. Biden Jr.",  "Convention Bounce for Joseph R. Biden Jr.")) %>%
  filter(state != "District of Columbia", 
         state != "ME-1", 
         state != "ME-2",
         state != "NE-1", 
         state != "NE-2", 
         state != "National") %>%
  mutate(date = as.Date(modeldate, format = "%m/%d/%Y")) %>%
  rename(year = cycle) %>%
  group_by(state) %>%
  summarize(avg_poll = mean(pct_estimate))## average poll since 10/8 per state for joe biden

# 2020 PREDICTION
pred_2020 <- predict(fit_state_poll, newdata = new_data_poll)
poll_pred_2020 <- tibble(state = new_data_poll$state, pred = pred_2020)

#############################
#### DEMO MODEL AND PRED ####
#############################
demog <- read_csv("~/Desktop/R studio/carine-h.github.io/data/demographic_1990-2018.csv")
pvstate_df    <- read_csv("~/Desktop/R studio/carine-h.github.io/data/popvote_bystate_1948-2016.csv")
pollstate_df  <- read_csv("~/Desktop/R studio/carine-h.github.io/data/pollavg_bystate_1968-2016.csv")
hispanic_2020 <- read_csv("~/Desktop/R studio/carine-h.github.io/data/csvData.csv")
race_2020 <- read_csv("~/Desktop/R studio/carine-h.github.io/data/demog_race.csv")
electoral_votes <- read_csv("~/Desktop/R studio/carine-h.github.io/data/electoralcollegevotes_1948-2020.csv")

# state names and abbreviations
pvstate_df$state <- state.abb[match(pvstate_df$state, state.name)]
pollstate_df$state <- state.abb[match(pollstate_df$state, state.name)]

dat <- pvstate_df %>% 
  full_join(pollstate_df %>% 
              filter(weeks_left == 10) %>% 
              group_by(year,party,state) %>% 
              summarise(avg_poll=mean(avg_poll)),
            by = c("year" ,"state")) %>%
  left_join(demog %>%
              select(-c("total")),
            by = c("year" ,"state"))
# demographics, poll numbers, and popular vote 

dat$region <- state.division[match(dat$state, state.abb)]
demog$region <- state.division[match(demog$state, state.abb)]

dat_change <- dat %>%
  group_by(state) %>%
  mutate(Asian_change = Asian - lag(Asian, order_by = year),
         Black_change = Black - lag(Black, order_by = year),
         Hispanic_change = Hispanic - lag(Hispanic, order_by = year),
         Indigenous_change = Indigenous - lag(Indigenous, order_by = year),
         White_change = White - lag(White, order_by = year),
         Female_change = Female - lag(Female, order_by = year),
         Male_change = Male - lag(Male, order_by = year),
         age20_change = age20 - lag(age20, order_by = year),
         age3045_change = age3045 - lag(age3045, order_by = year),
         age4565_change = age4565 - lag(age4565, order_by = year),
         age65_change = age65 - lag(age65, order_by = year)
  )



## MODEL
mod_demog_change <- lm(D_pv2p ~ Black_change + Hispanic_change + Asian_change +
                         as.factor(state), data = dat_change)


# UPDATED 2020
dat_2020 <- race_2020 %>%
  left_join(hispanic_2020, by = "State") %>%
  mutate(Hispanic = 100*HispanicPerc, 
         state = State, 
         White = 100*WhitePerc, 
         Asian = 100*AsianPerc, 
         Black = 100*BlackPerc) %>%
  select(state, Hispanic, White, Asian, Black) %>%
  mutate(state = state.abb[match(state,state.name)]) %>%
  na.omit() %>%
  mutate(year = 2020)

# 2018 demographics
dat_2018 <- demog %>%
  filter(year == 2018) 

# joining demographics
real_2020_change <- bind_rows(dat_2018, dat_2020)

# calculating percent changes in available demographic groups
## I used 0 percent change with populations that lacked demographic data (age and gender)
real_2020 <- real_2020_change %>%
  filter(year %in% c(2018, 2020)) %>%
  group_by(state) %>%
  mutate(Asian_change = Asian - lag(Asian, order_by = year), # CALCULATING CHANGES IN POPULATION
         Black_change = Black - lag(Black, order_by = year),
         Hispanic_change = Hispanic - lag(Hispanic, order_by = year),
         Indigenous_change = 0,
         White_change = White - lag(White, order_by = year),
         Female_change = 0,
         Male_change = 0,
         age20_change = 0,
         age3045_change = 0,
         age4565_change = 0,
         age65_change = 0) %>%
  filter(year == 2020)

real_2020 <- as.data.frame(real_2020)
rownames(real_2020) <- real_2020$state
real_2020 <- real_2020[state.abb, ]
real_2020$region <- state.division[match(real_2020$state, state.abb)]

#  2020 PREDICTION
demog_pred <- predict(mod_demog_change, newdata = real_2020) 

d <- tibble(demog_pred) %>%
  mutate(state = real_2020$state)


#############################
#### ECON MODEL AND PRED ####
#############################
library(readxl)
q2_2020 <- read_excel("/Users/carinehajjar/Desktop/R studio/carine-h.github.io/pred_data/q2_2020.xlsx") %>%
  na.omit()%>%
  mutate(state = state.abb[match(state,state.name)])
state_gdp <- read_csv("/Users/carinehajjar/Desktop/R studio/carine-h.github.io/pred_data/state_gdp_quarter.csv")

econ_update

# going to have to use yearly growth
state_econ <- econ_update %>%
  filter(Description == "Real GDP (millions of chained 2012 dollars)") %>%
  select(GeoName, '1997', '1998', '1999','2000', '2001', '2002', '2003', '2004', '2005', '2006', '2007', '2008', '2009', '2010', '2011', '2012', '2013', '2014', '2015', '2016', '2017', '2018')%>%
  melt(c("GeoName"), value.name = "GDP") %>%
  mutate(year = as.numeric(as.character(variable))) %>%
  rename(state = GeoName) %>%
  group_by(state) %>%
  mutate(gdp_growth= (GDP - lag(GDP, order_by = year))/lag(GDP, order_by = year)) %>%
  mutate(state = state.abb[match(state,state.name)]) %>%
  select(year, state, gdp_growth) %>%
  na.omit()
  
q3_econ <- pvstate_df %>%
  left_join(state_econ, by = c("year", "state")) %>%
  left_join(popvote_df, by = c("year")) %>%
  na.omit() %>%
  filter(party == "democrat")

# MODEL
econ_fit <- lm(D_pv2p ~ gdp_growth*incumbent + as.factor(state), data = q3_econ)
summary(econ_fit)

e <- econ_update %>%
  filter(Description == "Real GDP (millions of chained 2012 dollars)") %>%
  select(GeoName, '1997', '1998', '1999','2000', '2001', '2002', '2003', '2004', '2005', '2006', '2007', '2008', '2009', '2010', '2011', '2012', '2013', '2014', '2015', '2016', '2017', '2018')%>%
  melt(c("GeoName"), value.name = "GDP") %>%
  mutate(year = as.numeric(as.character(variable))) %>%
  rename(state = GeoName) %>%
  mutate(state = state.abb[match(state,state.name)]) %>%
  select(state, year, GDP) %>%
  filter(year == 2018)


gdp_2019 <- read_excel("/Users/carinehajjar/Desktop/R studio/carine-h.github.io/pred_data/state_2019_gdp.xlsx")%>%
  na.omit()%>%
  mutate(state = state.abb[match(state,state.name)]) %>%
  mutate(year = 2019)

# new data: ASSUMING SAME GDP GROWTH ACROSS STATES AS IS NATIONALLY 
data_econ <- q2_2020 %>%
  mutate(year = 2020) 

new_data_econ <- rbind(data_econ, gdp_2019) %>%
  mutate(incumbent = FALSE) %>%
  group_by(state) %>%
  mutate(gdp_growth = (GDP - lag(GDP, order_by = year))/lag(GDP, order_by = year)) %>%
  na.omit()

# 2020 PREDICTION
econ_pred <- predict(econ_fit, newdata = new_data_econ)

e2 <- tibble(pred = predict(econ_fit, newdata = new_data_econ), state = new_data_econ$state)



```

```{r}
#### ENSEMBLE ####
in_sample_poll <- as.data.frame(predict(fit_state_poll, newdata = new_data_poll, interval = "predict"))
in_sample_demo <- as.data.frame(predict(mod_demog_change, newdata = real_2020, interval = "predict"))
in_sample_econ <- as.data.frame(predict(econ_fit, newdata = new_data_econ, interval = "predict"))

# ensemble PREDICTION for 2020
ensemble <- 0.25*in_sample_econ$fit + 0.25*in_sample_demo$fit + 0.5*in_sample_poll$fit
ensemble_tibble <- tibble(pred = ensemble, state = new_data_poll$state) %>%
  mutate(state = state.abb[match(state,state.name)])

# electoral vote table:
p8 <- ensemble_tibble %>%
  left_join(electoral, by = "state") %>%
  mutate(state_win = case_when(pred > 50 ~ "Biden",
                            pred < 50 ~ "Trump")) %>%
  group_by(state_win) %>%
  summarise(electoral_votes = sum(`2020`)) %>%
  gt() %>%
  tab_header(title = md("**Poll-Heavy Ensemble: 2020 Electoral Vote Outcome Prediction**"), 
               subtitle = "Biden Wins") %>%
   cols_label(state_win = md("**Candidate**"),
               electoral_votes = md("**Total Electoral Votes**")) %>%
  tab_source_note(md("*Data: FiveThirtyEight*"))

p8 <- gt::gtsave(p8,
    filename = "images/eight.png")

# electoral map table:
p9 <- ensemble_tibble %>% 
  mutate(state = as.character(state)) %>% ##`statebins` needs state to be character, not factor!
  ggplot(aes(state = state, fill = (pred >= 50))) +
  geom_statebins() +
  theme_statebins() +
  labs(title = "2020 State Prediction",
       subtitle = "Poll-Heavy Ensemble Model",
       fill = "") +
  theme(legend.position = "none", 
        plot.subtitle = element_text(size = 10, hjust = 0.5),
        plot.title   = element_text(size = 12, hjust = 0.5, face = "bold"))

 ggsave("~/Desktop/R studio/carine-h.github.io/images/nine.png", p9)

```


I also predicted the democratic vote share per state: 

```{r}
#Predictive Intervals: POLL Ensemble
in_sample_polls <- predict(fit_state_poll, newdata = new_data_poll, interval = "predict") 
in_sample_demos <- predict(mod_demog_change, newdata = real_2020, interval = "predict")
in_sample_econs <- predict(econ_fit, newdata = new_data_econ, interval = "predict")

tab1 <- as.data.frame(0.5*in_sample_polls + 0.25*in_sample_demos + 0.25*in_sample_econs) %>%
  mutate(state = new_data_poll$state) %>%
  select(state, lwr, fit, upr) %>%
  mutate(lwr = round(lwr,  digits = 2)/100, 
         fit = round(fit, digits = 2)/100, 
         upr = round(upr, digits = 2)/100)%>%
  mutate(winner =  case_when(fit > .50 ~ "Biden",
                            fit < .50 ~ "Trump")) %>%
  gt() %>%
   tab_header(title = md("**Poll-Heavy Ensemble: Projected State Winners and Predictive Intervals for Democratic Vote Share Prediction**"), 
               subtitle = "95% Confidence Intervals") %>%
   fmt_percent(columns = c("lwr", "fit", "upr"), decimals = 1) %>%
   cols_label(lwr = md("**Lower Bound**"),
              fit = md("**Predicted Democratic Vote Share**"),
              upr = md("**Upper Bound**"), 
              state = md("**State**"), 
              winner = md("**Predicted Winner**")) %>%
  tab_source_note(md("*Data: BEA, FiveThirtyEight, World Population Review*")) 
             # convert to gt table
gt::gtsave(tab1,
    filename = "images/my_table_image.png")


# RMSE
as.data.frame(0.5*in_sample_polls + 0.25*in_sample_demos + 0.25*in_sample_econs) %>%
  mutate(state = new_data_poll$state) %>%
  select(state, lwr, fit, upr) %>%
  mutate(lwr = round(lwr,  digits = 2)/100, 
         fit = round(fit, digits = 2)/100, 
         upr = round(upr, digits = 2)/100)%>%
  mutate(winner =  case_when(fit > .50 ~ "Biden",
                            fit < .50 ~ "Trump")) %>%
  left_join(outcome_2020) %>%
  select(state, fit, D_pv2p) %>%
  mutate(diff = fit - D_pv2p) %>%
  mutate(diff_sq = diff^2)%>%
  summarise(rmse = sqrt(mean(diff_sq)))


  
```


My predictive model was a weighted ensemble that combined a fundamentals model, a demographic model, and a polls model. The fundamentals and demographics model had 25% weights, each, while the polls model had a 50% weight, making this a **poll-heavy** predictive model. Below are the variables for each of the ensemble components:
- Fundamentals: annual state GDP growth and incumbency
- Demographics: state-by-state demographic changes in the Black, Hispanic, Asian, and White state populations
- Polls: state-by-state presidential polls from 1972 onward

I chose a poll-heavy model for a variety of reasons. First and foremost, I felt that fundamentals, a traditionally robust predictor of elections, would be less useful this time around. The economy, for instance, has gone into an unusual shock from the COVID crisis. Incumbency is also different this time around - there has never been such a polarizing president as Donald Trump. Therefore, I was weary to place heavy weight on fundamentals during such a singular election. I also only placed 25% of the ensemble weight on demographics because I did not want to generalize that demographic groups vote as a monolith. Moreover, I only factored in Black, White, Hispanic, and Asian groups (due to data availability) and did not want to throw the model off with a rather simplistic view of the country's makeup. 

So, why did I place so much weight on the polls? 

For one, I naively assumed that pollsters would have learned their lesson from 2016: they would have cracked the code on detecting Trump support. This assumption was obviously incorrect. I also chose a poll-heavy prediction because I hoped that polls would be a better indicator of electoral preference on the state level while all other traditional indicators are in flux. 

This model composition intuitively made sense  - the economy is a mess and Trump is a one-of-a-kind president so the best thing to do is rely on the polls. However, throughout the process, my gut told me it would be a close race, perhaps even a race that Trump would still win - while it's easy to feel that Biden is a more palatable candidate, I had to remind myself that Harvard's thought bubble is ultra-liberal. So many places around the country still sympathized with Trump and even felt good about his leadership over the past almost-four years. In this context, I hoped my model would reflect a close race.

It was a huge surprise when the model produced a Biden landslide. However, I stood by the logic of my model's parameters and hoped that they'd hold some kind of truth. Though I was wrong in the end, the gap between the model and reality signaled the following rather scary reality: there is really no good indicator for presidential outcomes in an election like 2020's. I chalk this up to deeper polarization and partisanship. No matter what the polls say or how much GDP grows/shrinks, it seems that Americans have made their minds up. Fundamentals, public opinion, and even demographics fall to the wayside when Americans have already chosen their "camps."

```{r}

# my prediction 
ensemble_tibble %>% 
  mutate(state = as.character(state)) %>% ##`statebins` needs state to be character, not factor!
  ggplot(aes(state = state, fill = (pred >= 50))) +
  geom_statebins() +
  theme_statebins() +
  labs(title = "2020 State Prediction",
       subtitle = "Poll-Heavy Ensemble Model",
       fill = "") +
  theme(legend.position = "none", 
        plot.subtitle = element_text(size = 10, hjust = 0.5),
        plot.title   = element_text(size = 12, hjust = 0.5, face = "bold"))

outcome_2020 %>% 
  mutate(state = state.abb[match(state, state.name)]) %>% 
  na.omit() %>%
  ggplot(aes(state = state, fill = (D_pv2p > .50))) +
  geom_statebins() +
  theme_statebins() +
  labs(title = "2020 Actual State Outcomes",
       fill = "") +
  theme(legend.position = "none", 
        plot.subtitle = element_text(size = 10, hjust = 0.5),
        plot.title   = element_text(size = 12, hjust = 0.5, face = "bold"))


# meh way of doing it 
plot(ensemble_tibble$pred, outcome_2020$D_pv2p[!outcome_2020$state == 'District of Columbia'], pch = '')
text(ensemble_tibble$pred, outcome_2020$D_pv2p[!outcome_2020$state == 'District of Columbia'], label = ensemble_tibble$state)

# better - points are yours and line is real

```



Notes on meeting
- public opinion poll data over election result to measure polarization 
  - look as shrinking of independent voters 
  - conduct survey of reason WHY people voted the way ttehy did 
- also look at pre-covid economy - restropesctiive hyptoehsis 
  - public opiiniion poll 
  
changes 
- interaction model iwth poalizatiiion 

RMSE but asos show direction of error 
  - reflects the size of error 
claassificationi accuracy - divide staate you got right over total
draw a histogram for the models 
see how prediction error correlates with other errors 


RMSE
- subtract columns 
- swuare difference of EACH value - nw column 
- take MEAAAN of squared error column 

